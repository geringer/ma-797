{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c3Q097QaVVvW"
   },
   "source": [
    "# MA 797 Homework #1\n",
    "\n",
    "Note: For this assignment, you will code all problems in python. You may\n",
    "use sci-kit learn unless specifically stated otherwise. When you email me your\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLdVOy81VRgT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_gaussian_classes(m, S, P, N):\n",
    "  \n",
    "  # m := 1xc matrix whose jth column corresponds to the mean of the jth class\n",
    "  # S := 1x1xc matrix whose jth corresponds to the covariance of the jth class\n",
    "  # P := x-demensional vecotr whose jth class is the priori prob of jth clas\n",
    "  # N := total number of data vectors to be generated\n",
    "  # Note: this may generate less than N values due to the floor function\n",
    "  \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for j in range(len(m)):\n",
    "    \n",
    "        t = np.random.multivariate_normal( m[j], S[j], int( N / len(m))) #int(P[j] * N / len(m)))\n",
    "        s = [j]*333\n",
    "\n",
    "        X.append(t)\n",
    "        y.append(s)\n",
    "    \n",
    "    X = np.vstack(X)\n",
    "    y = np.vstack(y).flatten()\n",
    "    return X, y\n",
    "\n",
    "def euclidean_classifier(m, X):\n",
    "  \n",
    "  # m := 1xc matrix whose ith column corresponds to the mean of the ith class\n",
    "  # X := 1xN matrix whose columns are the data vectors to be classified\n",
    "  \n",
    "    y_hat = []\n",
    "    confidence = []\n",
    "\n",
    "    for x in X:\n",
    "        A = []\n",
    "\n",
    "        for mean in m: \n",
    "            distance = (x - mean)\n",
    "            A.append( np.sqrt( np.dot( np.transpose(distance), distance ) ) )\n",
    "\n",
    "        guess = np.where(A == np.amin(A))\n",
    "        y_hat.append(guess)\n",
    "    \n",
    "        prob = 1.0 - np.amin(A) / np.linalg.norm(A)\n",
    "        confidence.append(prob)\n",
    "  \n",
    "    confidence_np = np.vstack(confidence)\n",
    "    y_hat_np = np.vstack(y_hat)\n",
    "#     print(len(y_hat), y_hat_np.shape)\n",
    "    return y_hat_np\n",
    "\n",
    "\n",
    "\n",
    "def mahalanobis_classifier(m, S, X):\n",
    "  \n",
    "  # m := 1xc matric whose ith column is the mean of the ith class\n",
    "  # S := 1x1 matrix corresponding to the matrix in the mahalonobis distance\n",
    "  # X := 1xN whos columns are data vectors to be classfied\n",
    "  \n",
    "    y_hat = []\n",
    "    confidence = []\n",
    "\n",
    "    for x in X:\n",
    "    \n",
    "        A = []\n",
    "\n",
    "        for i in range(len(m)):\n",
    "      \n",
    "            mean = m[i]\n",
    "            distance = (x - mean)\n",
    "            right = np.dot( np.linalg.inv(S[i,:,:]) , distance ) \n",
    "            A.append( np.sqrt( np.dot( np.transpose(distance), right ) ) )\n",
    "      \n",
    "        guess = np.where(A == np.amin(A))\n",
    "        y_hat.append(guess)\n",
    "    \n",
    "        prob = 1.0 - np.amin(A) / np.linalg.norm(A)\n",
    "        confidence.append(prob)\n",
    "    \n",
    "    confidence_np = np.vstack(confidence)\n",
    "    y_hat_np = np.vstack(y_hat)\n",
    "\n",
    "    return y_hat_np\n",
    "\n",
    "\n",
    "\n",
    "def bayesian_classifier(m, S, P, X):\n",
    "  \n",
    "  # m := lxc matrix whose jth column corresponds to the mean of the jth class\n",
    "  # S := cxlxl matrix whose jth corresponds to the covariance of the jth class\n",
    "  # P := x-demensional vecotr whose jth class is the priori prob of jth clas\n",
    "  # N := total number of data vectors to be generated\n",
    "  \n",
    "    y_hat = []\n",
    "    confidence = []\n",
    "\n",
    "    for x in X:\n",
    "\n",
    "        A = []\n",
    "\n",
    "        for i in range(len(m)):\n",
    "      \n",
    "            mean = m[i]\n",
    "            distance = (x - mean)\n",
    "            not_dot = np.matmul(np.linalg.inv(S[i,:,:]), distance)\n",
    "            exp_component = np.dot( np.transpose(distance), not_dot)\n",
    "\n",
    "            det_S_inv = np.linalg.det(np.linalg.inv(S[i,:,:]))\n",
    "            denominator = np.sqrt(2 * np.pi * det_S_inv) \n",
    "            value  = np.exp(-1/2 * exp_component) / denominator\n",
    "            A.append(P[i] * value)\n",
    "      \n",
    "        guess = np.where(A == np.amax(A))\n",
    "        y_hat.append(guess)\n",
    "\n",
    "        prob = 1.0 - np.amin(A) / np.linalg.norm(A)\n",
    "        confidence.append(prob)\n",
    "        \n",
    "    confidence_np = np.vstack(confidence)\n",
    "    y_hat_np = np.vstack(y_hat)\n",
    "\n",
    "    return y_hat_np\n",
    "\n",
    "def calculate_error(y_guess, y_truth):\n",
    "    if y_guess.size != y_truth.size:\n",
    "        print(\"Array size mismatch\")\n",
    "        print(len(y_guess), len(y_truth))\n",
    "\n",
    "    total = len(y_guess)\n",
    "    sum = 0.0\n",
    "\n",
    "    for i in range(total):\n",
    "#         print(i, y_guess[i], y_truth[i])\n",
    "        if y_guess[i] != y_truth[i]:\n",
    "            sum += 1.0\n",
    "\n",
    "    error = sum / float(total)\n",
    "\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjyeKCCVmBMH"
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "Generate 2 data sets, X (training set) and X1 (test set), each consisting of N = 1000 3-d vectors that stem from three equiprobable classes w1, w2, and w3.\n",
    "\n",
    "The classes are modeled by Gaussian distributions with means m1 = [0, 0, 0]T, m2 = [1, 2, 2]T,and m3 = [3, 3, 4]T, respectively; their covariance matrices are S1 = S2 = S3 = 0.8I.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gasWDa8haap9"
   },
   "outputs": [],
   "source": [
    "m1 = np.array([0, 0, 0])\n",
    "m2 = np.array([1, 2, 2])\n",
    "m3 = np.array([3, 3, 4])\n",
    "m = [m1, m2, m3]\n",
    "\n",
    "s1 = np.array([\n",
    "    [.8, 0, 0], \n",
    "    [0, .8, 0], \n",
    "    [0, 0, .8]])\n",
    "\n",
    "s2 = s1\n",
    "s3 = s1\n",
    "S = np.stack([s1, s2, s3])\n",
    "\n",
    "P = np.array([1, 1, 1])\n",
    "\n",
    "N = 1000\n",
    "\n",
    "X_train, y_train = generate_gaussian_classes(m, S, P, N)\n",
    "X_test, y_test = generate_gaussian_classes(m, S, P, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxrHStcXhSwX"
   },
   "source": [
    "### Problem 1(a)\n",
    "Using X, compute the maximum likelihood estimates of the mean values and the co- variance matrices of the distributions of the three classes. Since the covariance matrices are known to the the same, estimate them for each class and compute their average. Use the latter as the estimate of the (common) covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfDNVXMxhRb8"
   },
   "outputs": [],
   "source": [
    "m1_train = np.mean(X_train[0:333], axis = 0)\n",
    "m2_train = np.mean(X_train[333:666], axis = 0)\n",
    "m3_train = np.mean(X_train[666:999], axis = 0)\n",
    "\n",
    "m_train = [m1_train, m2_train, m3_train]\n",
    "\n",
    "s1_train = np.cov(X_train[0:333], rowvar = False)\n",
    "s2_train = np.cov(X_train[333:666], rowvar = False)\n",
    "s3_train = np.cov(X_train[666:999], rowvar = False)\n",
    "\n",
    "S_train = np.stack([s1_train, s2_train, s3_train])\n",
    "S_common = np.mean(S_train, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jecdBjMrhlXY"
   },
   "source": [
    "### Problem 1(b)\n",
    "\n",
    "Use the Euclidean distance classifier to classify the points of X1 based on the ML estimates computed before. (Program your own classifier function.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "juuIOB1-hqaO"
   },
   "outputs": [],
   "source": [
    "y_hat_euclidean = euclidean_classifier(m_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DSHUEMc1hrPc"
   },
   "source": [
    "### Problem 1(c)\n",
    "\n",
    "Use the Mahalonobis distance classifier to classify the points of X1 based on the ML estimates computes before. (Program your own classifier function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQoEXNuLhvFQ"
   },
   "outputs": [],
   "source": [
    "y_hat_mahalanobis = mahalanobis_classifier(m_train, S_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2zBPX9gxhxTC"
   },
   "source": [
    "### Problem 1(d)\n",
    "\n",
    "Use the Bayesian classifier to classify the points of X1 basedon the ML estimates com- puted before. (Program your own classifier function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7D1Kl7o7h3df"
   },
   "outputs": [],
   "source": [
    "y_hat_bayes = bayesian_classifier(m_train, S_train, P, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yKHwZPlh3xH"
   },
   "source": [
    "### Problem 1(e)\n",
    "\n",
    "For each case, compute the error probability and compare the results (Why do all classifiers result in the same performance?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "_0ZXM-Uhh8gk",
    "outputId": "75517e84-2285-4f47-c291-b0372fffc7c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean Classifer has an error rate of  4.604604604604605 %\n",
      "The Mahalanobis Classifer has an error rate of  5.005005005005005 %\n",
      "The Bayesian Classifer has an error rate of  5.105105105105105 %\n"
     ]
    }
   ],
   "source": [
    "euclidean_error = calculate_error(y_hat_euclidean, y_test)\n",
    "print(\"The Euclidean Classifer has an error rate of \", euclidean_error * 100, \"%\")\n",
    "\n",
    "mahalanobis_error = calculate_error(y_hat_mahalanobis, y_test)\n",
    "print(\"The Mahalanobis Classifer has an error rate of \", mahalanobis_error * 100, \"%\")\n",
    "\n",
    "bayes_error = calculate_error(y_hat_bayes, y_test)\n",
    "print(\"The Bayesian Classifer has an error rate of \", bayes_error * 100, \"%\")\n",
    "\n",
    "###INCLUDE ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "02cSBNMNowEf"
   },
   "source": [
    "## Problem 2\n",
    "\n",
    "Repeat exercise (1) for (a) - (e) but now assume P1 = 0.5, P2 = P3 = 0.25\n",
    "Generate X and X1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7o4v2vVmacHi"
   },
   "outputs": [],
   "source": [
    "m1 = np.array([0, 0, 0])\n",
    "m2 = np.array([1, 2, 2])\n",
    "m3 = np.array([3, 3, 4])\n",
    "m = [m1, m2, m3]\n",
    "\n",
    "s1 = np.array([\n",
    "    [.8, 0, 0], \n",
    "    [0, .8, 0], \n",
    "    [0, 0, .8]])\n",
    "\n",
    "s2 = s1\n",
    "s3 = s1\n",
    "S = np.stack([s1, s2, s3])\n",
    "\n",
    "P = np.array([.5, .25, .25])\n",
    "\n",
    "N = 1000\n",
    "\n",
    "X_train, y_train = generate_gaussian_classes(m, S, P, N)\n",
    "X_test, y_test = generate_gaussian_classes(m, S, P, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZlIGDPU9m6mN"
   },
   "source": [
    "### Problem 2(a)\n",
    "Using X, compute the maximum likelihood estimates of the mean values and the co- variance matrices of the distributions of the three classes. Since the covariance matrices are known to the the same, estimate them for each class and compute their average. Use the latter as the estimate of the (common) covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DuNJYoBbm69N"
   },
   "outputs": [],
   "source": [
    "m1_train = np.mean(X_train[0:333], axis = 0)\n",
    "m2_train = np.mean(X_train[333:666], axis = 0)\n",
    "m3_train = np.mean(X_train[666:999], axis = 0)\n",
    "\n",
    "m_train = [m1_train, m2_train, m3_train]\n",
    "\n",
    "s1_train = np.cov(X_train[0:333], rowvar = False)\n",
    "s2_train = np.cov(X_train[333:666], rowvar = False)\n",
    "s3_train = np.cov(X_train[666:999], rowvar = False)\n",
    "\n",
    "S_train = np.stack([s1_train, s2_train, s3_train])\n",
    "S_common = np.mean(S_train, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GR2ub6tTm7wH"
   },
   "source": [
    "### Problem 2(b)\n",
    "\n",
    "Use the Euclidean distance classifier to classify the points of X1 based on the ML estimates computed before. (Program your own classifier function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4CHmvkgm79p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_euclidean = euclidean_classifier(m_train, X_test)\n",
    "y_hat_euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlLGLTKim8KS"
   },
   "source": [
    "### Problem 2(c)\n",
    "\n",
    "Use the Mahalonobis distance classifier to classify the points of X1 based on the ML estimates computes before. (Program your own classifier function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GY5MHBz4m8VV"
   },
   "outputs": [],
   "source": [
    "y_hat_mahalanobis = mahalanobis_classifier(m_train, S_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fTpSa_aZm8gx"
   },
   "source": [
    "### Problem 2(d)\n",
    "\n",
    "Use the Bayesian classifier to classify the points of X1 basedon the ML estimates com- puted before. (Program your own classifier function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlXzoiX-m8q6"
   },
   "outputs": [],
   "source": [
    "y_hat_bayes = bayesian_classifier(m_train, S_train, P, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8jzCF00Bm80e"
   },
   "source": [
    "### Problem 2(e)\n",
    "\n",
    "For each case, compute the error probability and compare the results (Why does the Bayesian classifier result in the best performance?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Uae2vc-gm8_b",
    "outputId": "1aff4548-3249-4d82-ffed-7e56259851d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean Classifer has an error rate of  6.306306306306306 %\n",
      "The Mahalanobis Classifer has an error rate of  6.606606606606606 %\n",
      "The Bayesian Classifer has an error rate of  6.506506506506507 %\n"
     ]
    }
   ],
   "source": [
    "euclidean_error = calculate_error(y_hat_euclidean, y_test)\n",
    "print(\"The Euclidean Classifer has an error rate of \", euclidean_error * 100, \"%\")\n",
    "\n",
    "mahalanobis_error = calculate_error(y_hat_mahalanobis, y_test)\n",
    "print(\"The Mahalanobis Classifer has an error rate of \", mahalanobis_error * 100, \"%\")\n",
    "\n",
    "bayes_error = calculate_error(y_hat_bayes, y_test)\n",
    "print(\"The Bayesian Classifer has an error rate of \", bayes_error * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b3P-CLplbU4g"
   },
   "source": [
    "## Problem 3\n",
    "Use the data from exercise (1)\n",
    "\n",
    "### Problem 3(a)\n",
    "Compute an estimate for the PDFs using Parzen windows (kernel density estimation),\n",
    "i.e., to compute p(x|w1) and p(x|w2). Note that parts (a) and (b) of this problem are\n",
    "connected in the choice of the window size h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RzQDk9kuWZ4-",
    "outputId": "9b39aaef-13e1-4223-8d8c-915e6f1c3fab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bayesian Classifer has an error rate of  13.813813813813812 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors.kde import KernelDensity\n",
    "\n",
    "parzen = KernelDensity(kernel='gaussian', bandwidth=.2).fit(X_train)\n",
    "P_test = np.exp(parzen.score_samples(X_test))\n",
    "\n",
    "y_hat_bayes = bayesian_classifier(m_train, S_train, P_test, X_test)\n",
    "\n",
    "bayes_error = calculate_error(y_hat_bayes[:,0], y_test)\n",
    "print(\"The Bayesian Classifer has an error rate of \", bayes_error * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPefqeiqrFCC"
   },
   "source": [
    "### Problem 3(b)\n",
    "\n",
    "Use the Bayesian classifier to classify the points of X1 based on the Parzen window estimation method. Use different values of h and choose the one that results in the best\n",
    "error performance of the classifier. Make a plot to justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "hnmzy0dc-aU3",
    "outputId": "9cb6922c-1ffc-47a7-c264-8fad05a8b6f0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8188ac53c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "A = []\n",
    "\n",
    "for i in range(2, 100):\n",
    "  \n",
    "    parzen = KernelDensity(kernel='gaussian', bandwidth= i / 50).fit(X_train)\n",
    "    P_test = np.exp(parzen.score_samples(X_test))\n",
    "    \n",
    "    y_hat = bayesian_classifier(m_train, S_train, P_test, X_test)\n",
    "\n",
    "    error = calculate_error(y_hat[:,0], y_test)\n",
    "\n",
    "    A.append(np.array([i, error]))\n",
    "  \n",
    "A = np.vstack(A)  \n",
    "lowest = np.where(A == np.amin(A[:,1]))\n",
    "\n",
    "\n",
    "plt.plot(A[:,0], A[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "dcOHyF-6NKMg",
    "outputId": "5b1a0ebf-6371-41ba-9eee-e6943ce61b4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Parzen window of 1.18 had the lowest error of 6.006006006006006 %\n"
     ]
    }
   ],
   "source": [
    "parzen = KernelDensity(kernel='gaussian', bandwidth= 59 / 50).fit(X_train)\n",
    "P_test = np.exp(parzen.score_samples(X_test))\n",
    "\n",
    "y_hat = bayesian_classifier(m_train, S_train, P_test, X_test)\n",
    "\n",
    "error = calculate_error(y_hat[:,0], y_test)\n",
    "\n",
    "print(\"The Parzen window of 1.18 had the lowest error of\", error * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgqgJeiZbbDL"
   },
   "source": [
    "## Problem 4\n",
    "\n",
    "Repeat exercise (3) but use k-NN instead of parzen windows. Use different\n",
    "values of k and choose the one that results in the best error performance of the classifier.\n",
    "Make a plot to justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "NfKeYfqwmV9H",
    "outputId": "19912f09-4c44-48eb-aeb6-c8cdc5efaa90"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvojIGKKgLyIyhijoi4iMIf8fgGQOfz1VhFkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81889f2ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "A = []\n",
    "\n",
    "for i in range(2, 500):\n",
    "  \n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train) \n",
    "\n",
    "    y_hat = knn.predict(X_test)\n",
    "\n",
    "    error = calculate_error(y_hat, y_test)\n",
    "\n",
    "    A.append(np.array([i, error]))\n",
    "  \n",
    "\n",
    "A = np.vstack(A)  \n",
    "lowest = np.where(A == np.amin(A[:,1]))\n",
    "\n",
    "\n",
    "plt.plot(A[:,0], A[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "colab_type": "code",
    "id": "9jrPcDRE1TP4",
    "outputId": "bf446936-9abd-44f3-9289-d936cada7494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 342-nearest neighbors had the lowest error of 5.905905905905906 %\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=342)\n",
    "knn.fit(X_train, y_train) \n",
    "\n",
    "y_hat = knn.predict(X_test)\n",
    "\n",
    "error = calculate_error(y_hat, y_test)\n",
    "\n",
    "print(\"The 342-nearest neighbors had the lowest error of\", error * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XNHXXYBU6gU_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Homework1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
