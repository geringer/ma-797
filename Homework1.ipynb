{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geringer/ma-797/blob/master/Homework1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3Q097QaVVvW",
        "colab_type": "text"
      },
      "source": [
        "# MA 797 Homework #1\n",
        "\n",
        "Note: For this assignment, you will code all problems in python. You may\n",
        "use sci-kit learn unless specifically stated otherwise. When you email me your\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLdVOy81VRgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_gaussian_classes(m, S, P, N):\n",
        "  \n",
        "  # m := 1xc matrix whose jth column corresponds to the mean of the jth class\n",
        "  # S := 1x1xc matrix whose jth corresponds to the covariance of the jth class\n",
        "  # P := x-demensional vecotr whose jth class is the priori prob of jth clas\n",
        "  # N := total number of data vectors to be generated\n",
        "  # Note: this may generate less than N values due to the floor function\n",
        "  \n",
        "  X = []\n",
        "  y = []\n",
        "  \n",
        "  for j in range(len(m)):\n",
        "    \n",
        "    t = np.random.multivariate_normal( m[j], S[j], 333)#, int(P[j] * N))\n",
        "    s = [j]*333\n",
        "    \n",
        "    X.append(t)\n",
        "    y.append(s)\n",
        "    \n",
        "  X = np.vstack(X)\n",
        "  y = np.vstack(y).flatten()\n",
        "  return X, y\n",
        "\n",
        "def euclidean_classifier(m, X):\n",
        "  \n",
        "  # m := 1xc matrix whose ith column corresponds to the mean of the ith class\n",
        "  # X := 1xN matrix whose columns are the data vectors to be classified\n",
        "  \n",
        "  y_hat = []\n",
        "  confidence = []\n",
        "  \n",
        "  for x in X:\n",
        "    A = []\n",
        "    \n",
        "    for mean in m: \n",
        "      distance = (x - mean)\n",
        "      A.append( np.sqrt( np.dot( np.transpose(distance), distance ) ) )\n",
        "    \n",
        "    guess = np.where(A == np.amin(A))\n",
        "    y_hat.append(guess)\n",
        "    \n",
        "    prob = 1.0 - np.amin(A) / np.linalg.norm(A)\n",
        "    confidence.append(prob)\n",
        "  \n",
        "  confidence_np = np.vstack(confidence)\n",
        "  y_hat_np = np.vstack(y_hat)\n",
        "  \n",
        "  return np.hstack([y_hat_np, confidence_np])\n",
        "\n",
        "\n",
        "\n",
        "def mahalanobis_classifier(m, S, X):\n",
        "  \n",
        "  # m := 1xc matric whose ith column is the mean of the ith class\n",
        "  # S := 1x1 matrix corresponding to the matrix in the mahalonobis distance\n",
        "  # X := 1xN whos columns are data vectors to be classfied\n",
        "  \n",
        "  y_hat = []\n",
        "  confidence = []\n",
        "  \n",
        "  for x in X:\n",
        "    \n",
        "    A = []\n",
        "    \n",
        "    for i in range(len(m)):\n",
        "      \n",
        "      mean = m[i]\n",
        "      distance = (x - mean)\n",
        "      right = np.dot( np.linalg.inv(S[i,:,:]) , distance ) \n",
        "      A.append( np.sqrt( np.dot( np.transpose(distance), right ) ) )\n",
        "      \n",
        "    guess = np.where(A == np.amin(A))\n",
        "    y_hat.append(guess)\n",
        "    \n",
        "    prob = 1.0 - np.amin(A) / np.linalg.norm(A)\n",
        "    confidence.append(prob)\n",
        "    \n",
        "  confidence_np = np.vstack(confidence)\n",
        "  y_hat_np = np.vstack(y_hat)\n",
        "  \n",
        "  return np.hstack([y_hat_np, confidence_np])\n",
        "\n",
        "\n",
        "\n",
        "def bayesian_classifier(m, S, P, X):\n",
        "  \n",
        "  # m := lxc matrix whose jth column corresponds to the mean of the jth class\n",
        "  # S := cxlxl matrix whose jth corresponds to the covariance of the jth class\n",
        "  # P := x-demensional vecotr whose jth class is the priori prob of jth clas\n",
        "  # N := total number of data vectors to be generated\n",
        "  \n",
        "  y_hat = []\n",
        "  confidence = []\n",
        "  \n",
        "  for x in X:\n",
        "    \n",
        "    A = []\n",
        "    \n",
        "    for i in range(len(m)):\n",
        "      \n",
        "      mean = m[i]\n",
        "      distance = (x - mean)\n",
        "      not_dot = np.matmul(np.linalg.inv(S[i,:,:]), distance)\n",
        "      exp_component = np.dot( np.transpose(distance), not_dot)\n",
        "      \n",
        "      det_S_inv = np.linalg.det(np.linalg.inv(S[i,:,:]))\n",
        "      denominator = np.sqrt(2 * np.pi * det_S_inv) \n",
        "      value  = np.exp(-1/2 * exp_component) / denominator\n",
        "      A.append(P[i] * value)\n",
        "      \n",
        "    guess = np.where(A == np.amax(A))\n",
        "    y_hat.append(guess)\n",
        "    \n",
        "    prob = 1.0 - np.amin(A) / np.linalg.norm(A)\n",
        "    confidence.append(prob)\n",
        "    \n",
        "  confidence_np = np.vstack(confidence)\n",
        "  y_hat_np = np.vstack(y_hat)\n",
        "  \n",
        "  return np.hstack([y_hat_np, confidence_np])\n",
        "\n",
        "def calculate_error(y_guess, y_truth):\n",
        "  \n",
        "  if y_guess.size != y_truth.size:\n",
        "    print(\"Array size mismatch\")\n",
        "\n",
        "  total = len(y_guess)\n",
        "  sum = 0.0\n",
        "  \n",
        "  for i in range(total):\n",
        "    if y_guess[i] != y_truth[i]:\n",
        "      sum += 1.0\n",
        "      \n",
        "  error = sum / float(total)\n",
        "  \n",
        "  return error\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjyeKCCVmBMH",
        "colab_type": "text"
      },
      "source": [
        "## Problem 1\n",
        "\n",
        "Generate 2 data sets, X (training set) and X1 (test set), each consisting of N = 1000 3-d vectors that stem from three equiprobable classes w1, w2, and w3.\n",
        "\n",
        "The classes are modeled by Gaussian distributions with means m1 = [0, 0, 0]T, m2 = [1, 2, 2]T,and m3 = [3, 3, 4]T, respectively; their covariance matrices are S1 = S2 = S3 = 0.8I.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gasWDa8haap9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m1 = np.array([0, 0, 0])\n",
        "m2 = np.array([1, 2, 2])\n",
        "m3 = np.array([3, 3, 4])\n",
        "m = [m1, m2, m3]\n",
        "\n",
        "s1 = np.array([\n",
        "    [.8, 0, 0], \n",
        "    [0, .8, 0], \n",
        "    [0, 0, .8]])\n",
        "\n",
        "s2 = s1\n",
        "s3 = s1\n",
        "S = np.stack([s1, s2, s3])\n",
        "\n",
        "P = np.array([1, 1, 1])\n",
        "\n",
        "N = 1000\n",
        "\n",
        "X_train, y_train = generate_gaussian_classes(m, S, P, N)\n",
        "X_test, y_test = generate_gaussian_classes(m, S, P, N)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxrHStcXhSwX",
        "colab_type": "text"
      },
      "source": [
        "### Problem 1(a)\n",
        "Using X, compute the maximum likelihood estimates of the mean values and the co- variance matrices of the distributions of the three classes. Since the covariance matrices are known to the the same, estimate them for each class and compute their average. Use the latter as the estimate of the (common) covariance matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfDNVXMxhRb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m1_train = np.mean(X_train[0:333], axis = 0)\n",
        "m2_train = np.mean(X_train[333:666], axis = 0)\n",
        "m3_train = np.mean(X_train[666:999], axis = 0)\n",
        "\n",
        "m_train = [m1_train, m2_train, m3_train]\n",
        "\n",
        "s1_train = np.cov(X_train[0:333], rowvar = False)\n",
        "s2_train = np.cov(X_train[333:666], rowvar = False)\n",
        "s3_train = np.cov(X_train[666:999], rowvar = False)\n",
        "\n",
        "S_train = np.stack([s1_train, s2_train, s3_train])\n",
        "S_common = np.mean(S_train, axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jecdBjMrhlXY",
        "colab_type": "text"
      },
      "source": [
        "### Problem 1(b)\n",
        "\n",
        "Use the Euclidean distance classifier to classify the points of X1 based on the ML estimates computed before. (Program your own classifier function.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juuIOB1-hqaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_euclidean = euclidean_classifier(m_train, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSHUEMc1hrPc",
        "colab_type": "text"
      },
      "source": [
        "### Problem 1(c)\n",
        "\n",
        "Use the Mahalonobis distance classifier to classify the points of X1 based on the ML estimates computes before. (Program your own classifier function.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQoEXNuLhvFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_mahalanobis = mahalanobis_classifier(m_train, S_train, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zBPX9gxhxTC",
        "colab_type": "text"
      },
      "source": [
        "### Problem 1(d)\n",
        "\n",
        "Use the Bayesian classifier to classify the points of X1 basedon the ML estimates com- puted before. (Program your own classifier function.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D1Kl7o7h3df",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_bayes = bayesian_classifier(m_train, S_train, P, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yKHwZPlh3xH",
        "colab_type": "text"
      },
      "source": [
        "### Problem 1(e)\n",
        "\n",
        "For each case, compute the error probability and compare the results (Why do all classifiers result in the same performance?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0ZXM-Uhh8gk",
        "colab_type": "code",
        "outputId": "75517e84-2285-4f47-c291-b0372fffc7c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "euclidean_error = calculate_error(y_hat_euclidean[:,0], y_test)\n",
        "print(\"The Euclidean Classifer has an error rate of \", euclidean_error * 100, \"%\")\n",
        "\n",
        "mahalanobis_error = calculate_error(y_hat_mahalanobis[:,0], y_test)\n",
        "print(\"The Mahalanobis Classifer has an error rate of \", mahalanobis_error * 100, \"%\")\n",
        "\n",
        "bayes_error = calculate_error(y_hat_bayes[:,0], y_test)\n",
        "print(\"The Bayesian Classifer has an error rate of \", bayes_error * 100, \"%\")\n",
        "\n",
        "###INCLUDE ANALYSIS\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Euclidean Classifer has an error rate of  5.7057057057057055 %\n",
            "The Mahalanobis Classifer has an error rate of  5.805805805805806 %\n",
            "The Bayesian Classifer has an error rate of  5.605605605605605 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02cSBNMNowEf",
        "colab_type": "text"
      },
      "source": [
        "## Problem 2\n",
        "\n",
        "Repeat exercise (1) for (a) - (e) but now assume P1 = 0.5, P2 = P3 = 0.25\n",
        "Generate X and X1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o4v2vVmacHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m1 = np.array([0, 0, 0])\n",
        "m2 = np.array([1, 2, 2])\n",
        "m3 = np.array([3, 3, 4])\n",
        "m = [m1, m2, m3]\n",
        "\n",
        "s1 = np.array([\n",
        "    [.8, 0, 0], \n",
        "    [0, .8, 0], \n",
        "    [0, 0, .8]])\n",
        "\n",
        "s2 = s1\n",
        "s3 = s1\n",
        "S = np.stack([s1, s2, s3])\n",
        "\n",
        "P = np.array([.5, .25, .25])\n",
        "\n",
        "N = 1000\n",
        "\n",
        "X_train, y_train = generate_gaussian_classes(m, S, P, N)\n",
        "X_test, y_test = generate_gaussian_classes(m, S, P, N)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlIGDPU9m6mN",
        "colab_type": "text"
      },
      "source": [
        "### Problem 2(a)\n",
        "Using X, compute the maximum likelihood estimates of the mean values and the co- variance matrices of the distributions of the three classes. Since the covariance matrices are known to the the same, estimate them for each class and compute their average. Use the latter as the estimate of the (common) covariance matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuNJYoBbm69N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m1_train = np.mean(X_train[0:333], axis = 0)\n",
        "m2_train = np.mean(X_train[333:666], axis = 0)\n",
        "m3_train = np.mean(X_train[666:999], axis = 0)\n",
        "\n",
        "m_train = [m1_train, m2_train, m3_train]\n",
        "\n",
        "s1_train = np.cov(X_train[0:333], rowvar = False)\n",
        "s2_train = np.cov(X_train[333:666], rowvar = False)\n",
        "s3_train = np.cov(X_train[666:999], rowvar = False)\n",
        "\n",
        "S_train = np.stack([s1_train, s2_train, s3_train])\n",
        "S_common = np.mean(S_train, axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR2ub6tTm7wH",
        "colab_type": "text"
      },
      "source": [
        "### Problem 2(b)\n",
        "\n",
        "Use the Euclidean distance classifier to classify the points of X1 based on the ML estimates computed before. (Program your own classifier function.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4CHmvkgm79p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_euclidean = euclidean_classifier(m_train, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlLGLTKim8KS",
        "colab_type": "text"
      },
      "source": [
        "### Problem 2(c)\n",
        "\n",
        "Use the Mahalonobis distance classifier to classify the points of X1 based on the ML estimates computes before. (Program your own classifier function.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY5MHBz4m8VV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_mahalanobis = mahalanobis_classifier(m_train, S_train, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTpSa_aZm8gx",
        "colab_type": "text"
      },
      "source": [
        "### Problem 2(d)\n",
        "\n",
        "Use the Bayesian classifier to classify the points of X1 basedon the ML estimates com- puted before. (Program your own classifier function.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlXzoiX-m8q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_bayes = bayesian_classifier(m_train, S_train, P, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jzCF00Bm80e",
        "colab_type": "text"
      },
      "source": [
        "### Problem 2(e)\n",
        "\n",
        "For each case, compute the error probability and compare the results (Why does the Bayesian classifier result in the best performance?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uae2vc-gm8_b",
        "colab_type": "code",
        "outputId": "1aff4548-3249-4d82-ffed-7e56259851d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "euclidean_error = calculate_error(y_hat_euclidean[:,0], y_test)\n",
        "print(\"The Euclidean Classifer has an error rate of \", euclidean_error * 100, \"%\")\n",
        "\n",
        "mahalanobis_error = calculate_error(y_hat_mahalanobis[:,0], y_test)\n",
        "print(\"The Mahalanobis Classifer has an error rate of \", mahalanobis_error * 100, \"%\")\n",
        "\n",
        "bayes_error = calculate_error(y_hat_bayes[:,0], y_test)\n",
        "print(\"The Bayesian Classifer has an error rate of \", bayes_error * 100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Euclidean Classifer has an error rate of  5.805805805805806 %\n",
            "The Mahalanobis Classifer has an error rate of  5.805805805805806 %\n",
            "The Bayesian Classifer has an error rate of  6.206206206206207 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3P-CLplbU4g",
        "colab_type": "text"
      },
      "source": [
        "## Problem 3\n",
        "Use the data from exercise (1)\n",
        "\n",
        "### Problem 3(a)\n",
        "Compute an estimate for the PDFs using Parzen windows (kernel density estimation),\n",
        "i.e., to compute p(x|w1) and p(x|w2). Note that parts (a) and (b) of this problem are\n",
        "connected in the choice of the window size h."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzQDk9kuWZ4-",
        "colab_type": "code",
        "outputId": "9b39aaef-13e1-4223-8d8c-915e6f1c3fab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.neighbors.kde import KernelDensity\n",
        "\n",
        "parzen = KernelDensity(kernel='gaussian', bandwidth=1).fit(X_train)\n",
        "P_train = parzen.score_samples(X_train)\n",
        "\n",
        "y_hat_bayes = bayesian_classifier(m_train, S_train, P_train, X_test)\n",
        "\n",
        "bayes_error = calculate_error(y_hat_bayes[:,0], y_test)\n",
        "print(\"The Bayesian Classifer has an error rate of \", bayes_error * 100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Bayesian Classifer has an error rate of  100.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPefqeiqrFCC",
        "colab_type": "text"
      },
      "source": [
        "### Problem 3(b)\n",
        "\n",
        "Use the Bayesian classifier to classify the points of X1 based on the Parzen window estimation method. Use different values of h and choose the one that results in the best\n",
        "error performance of the classifier. Make a plot to justify your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnmzy0dc-aU3",
        "colab_type": "code",
        "outputId": "9cb6922c-1ffc-47a7-c264-8fad05a8b6f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "A = []\n",
        "\n",
        "for i in range(2, 50):\n",
        "  \n",
        "  parzen = KernelDensity(kernel='gaussian', bandwidth=1).fit(X_train)\n",
        "  P_train = parzen.score_samples(X_train)\n",
        "  \n",
        "  y_hat = bayesian_classifier(m_train, S_train, P_train, X_test)\n",
        "  \n",
        "  error = calculate_error(y_hat[:,0], y_test)\n",
        "  \n",
        "  A.append(np.array([i, error]))\n",
        "  \n",
        "\n",
        "A = np.vstack(A)  \n",
        "lowest = np.where(A == np.amin(A[:,1]))\n",
        "\n",
        "\n",
        "plt.plot(A[:,0], A[:,1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADj5JREFUeJzt3H+s3Xddx/Hni7UVI5DCel1mWyiE\nGilmjnkpQ8DVJWI3kcpikAXDtpj0D0aCiYsZ8sfCyEIUVFwkkKrNmOjmgoBFZ8YyRuYfDHfrfm8O\nCgHXbtJLxsBlCWTj7R/nWzzctL237bk9d30/H8lNz/fz/Z5zP+eznOf93u85d6kqJEk9PG/aE5Ak\nnTxGX5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjRh9SWrE6EtSI6umPYGF1q1bV5s2bZr2NCTpOWXv\n3r3fqaqZxY5bcdHftGkTc3Nz056GJD2nJPnWUo7z8o4kNWL0JakRoy9JjRh9SWrE6EtSI0Zfkhox\n+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0Y\nfUlqxOhLUiNGX5IaMfqS1IjRl6RGFo1+kt1JDiZ54Aj7k+TaJPuS3JfknAX7X5Rkf5K/mtSkJUnH\nZyln+tcB24+y/wJg8/C1E/j4gv0fBO44nslJkiZr0ehX1R3AE0c5ZAdwfY3cCaxNciZAkl8GzgC+\nMInJSpJOzCSu6a8HHh3b3g+sT/I84M+AKybwPSRJE7Ccb+S+G7i5qvYvdmCSnUnmkszNz88v45Qk\nqbdVE3iMA8DGse0Nw9jrgTcleTfwAmBNkqeq6sqFD1BVu4BdALOzszWBOUmSDmMS0d8DvCfJjcDr\ngO9V1ePAOw8dkORSYPZwwZcknTyLRj/JDcA2YF2S/cBVwGqAqvoEcDNwIbAPeBq4bLkmK0k6MYtG\nv6ouXmR/AZcvcsx1jD76KUmaIv8iV5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjRh9SWrE6EtSI0Zf\nkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMv\nSY0YfUlqxOhLUiNGX5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjRh9SWpk0egn2Z3kYJIHjrA/Sa5N\nsi/JfUnOGcbPTvLlJA8O47876clLko7NUs70rwO2H2X/BcDm4Wsn8PFh/GngXVX16uH+H02y9vin\nKkk6UasWO6Cq7kiy6SiH7ACur6oC7kyyNsmZVfXVscd4LMlBYAZ48gTnLEk6TpO4pr8eeHRse/8w\n9mNJtgJrgK9P4PtJko7Tsr+Rm+RM4O+Ay6rqR0c4ZmeSuSRz8/Pzyz0lSWprEtE/AGwc294wjJHk\nRcC/Au+vqjuP9ABVtauqZqtqdmZmZgJTkiQdziSivwd41/ApnnOB71XV40nWAJ9ldL3/0xP4PpKk\nE7ToG7lJbgC2AeuS7AeuAlYDVNUngJuBC4F9jD6xc9lw17cDvwqcnuTSYezSqrpngvOXJB2DpXx6\n5+JF9hdw+WHGPwV86vinJkmaNP8iV5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjRh9SWrE6EtSI0Zf\nkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMv\nSY0YfUlqxOhLUiNGX5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjSwa/SS7kxxM8sAR9ifJtUn2Jbkv\nyTlj+y5J8rXh65JJTlySdOyWcqZ/HbD9KPsvADYPXzuBjwMkeQlwFfA6YCtwVZIXn8hkJUknZtHo\nV9UdwBNHOWQHcH2N3AmsTXIm8BvArVX1RFV9F7iVo//wkCQts1UTeIz1wKNj2/uHsSONL5sPfP5B\nHnrs+8v5LSRp2Wz5uRdx1W+9elm/x4p4IzfJziRzSebm5+enPR1JOmVN4kz/ALBxbHvDMHYA2LZg\n/EuHe4Cq2gXsApidna3jnchy/4SUpOe6SZzp7wHeNXyK51zge1X1OHAL8OYkLx7ewH3zMCZJmpJF\nz/ST3MDojH1dkv2MPpGzGqCqPgHcDFwI7AOeBi4b9j2R5IPAXcNDXV1VR3tDWJK0zBaNflVdvMj+\nAi4/wr7dwO7jm5okadJWxBu5kqSTw+hLUiNGX5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjRh9SWrE\n6EtSI0Zfkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi\n9CWpEaMvSY0YfUlqxOhLUiNGX5IaMfqS1IjRl6RGjL4kNWL0JamRJUU/yfYkjyTZl+TKw+x/WZLb\nktyX5EtJNozt+9MkDyZ5OMm1STLJJyBJWrpFo5/kNOBjwAXAFuDiJFsWHPYR4PqqOgu4GvjQcN9f\nAd4AnAX8IvBa4LyJzV6SdEyWcqa/FdhXVd+oqh8CNwI7FhyzBfjicPv2sf0FPB9YA/wUsBr49olO\nWpJ0fJYS/fXAo2Pb+4excfcCFw233wa8MMnpVfVlRj8EHh++bqmqh09sypKk4zWpN3KvAM5Lcjej\nyzcHgGeTvBJ4FbCB0Q+K85O8aeGdk+xMMpdkbn5+fkJTkiQttJToHwA2jm1vGMZ+rKoeq6qLquo1\nwPuHsScZnfXfWVVPVdVTwL8Br1/4DapqV1XNVtXszMzMcT4VSdJilhL9u4DNSV6eZA3wDmDP+AFJ\n1iU59FjvA3YPt/+b0W8Aq5KsZvRbgJd3JGlKFo1+VT0DvAe4hVGwb6qqB5NcneStw2HbgEeSfBU4\nA7hmGP808HXgfkbX/e+tqs9P9ilIkpYqVTXtOfyE2dnZmpubm/Y0JOk5Jcneqppd7Dj/IleSGjH6\nktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqxOhLUiNGX5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjRh9\nSWrE6EtSI0Zfkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+\nJDVi9CWpEaMvSY0sKfpJtid5JMm+JFceZv/LktyW5L4kX0qyYWzfS5N8IcnDSR5Ksmly05ckHYtF\no5/kNOBjwAXAFuDiJFsWHPYR4PqqOgu4GvjQ2L7rgQ9X1auArcDBSUxcknTslnKmvxXYV1XfqKof\nAjcCOxYcswX44nD79kP7hx8Oq6rqVoCqeqqqnp7IzCVJx2wp0V8PPDq2vX8YG3cvcNFw+23AC5Oc\nDvw88GSSzyS5O8mHh98cJElTMKk3cq8AzktyN3AecAB4FlgFvGnY/1rgFcClC++cZGeSuSRz8/Pz\nE5qSJGmhpUT/ALBxbHvDMPZjVfVYVV1UVa8B3j+MPcnot4J7hktDzwCfA85Z+A2qaldVzVbV7MzM\nzHE+FUnSYpYS/buAzUlenmQN8A5gz/gBSdYlOfRY7wN2j913bZJDJT8feOjEpy1JOh6LRn84Q38P\ncAvwMHBTVT2Y5Ookbx0O2wY8kuSrwBnANcN9n2V0aee2JPcDAf564s9CkrQkqappz+EnzM7O1tzc\n3LSnIUnPKUn2VtXsYsf5F7mS1IjRl6RGjL4kNWL0JakRoy9JjRh9SWrE6EtSI0Zfkhox+pLUiNGX\npEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqxOhL\nUiNGX5IaMfqS1Eiqatpz+AlJ5oFvDZvrgO9McTrT1v35g2sArgG4BrD4GrysqmYWe5AVF/1xSeaq\nanba85iW7s8fXANwDcA1gMmtgZd3JKkRoy9Jjaz06O+a9gSmrPvzB9cAXANwDWBCa7Cir+lLkiZr\npZ/pS5ImaEVGP8n2JI8k2ZfkymnP52RIsjvJwSQPjI29JMmtSb42/Pviac5xuSXZmOT2JA8leTDJ\ne4fxFuuQ5PlJ/iPJvcPz/8Aw/vIkXxleD/+YZM2057rckpyW5O4k/zJst1qDJN9Mcn+Se5LMDWMT\neR2suOgnOQ34GHABsAW4OMmW6c7qpLgO2L5g7ErgtqraDNw2bJ/KngH+sKq2AOcClw//7busww+A\n86vql4Czge1JzgX+BPiLqnol8F3g96c4x5PlvcDDY9sd1+DXqurssY9pTuR1sOKiD2wF9lXVN6rq\nh8CNwI4pz2nZVdUdwBMLhncAnxxufxL47ZM6qZOsqh6vqv8cbv8voxf9epqsQ408NWyuHr4KOB/4\n9DB+yj7/Q5JsAH4T+JthOzRbgyOYyOtgJUZ/PfDo2Pb+YayjM6rq8eH2/wBnTHMyJ1OSTcBrgK/Q\naB2Gyxr3AAeBW4GvA09W1TPDIR1eDx8F/gj40bB9Ov3WoIAvJNmbZOcwNpHXwapJzE7Lr6oqSYuP\nWiV5AfBPwB9U1fdHJ3ojp/o6VNWzwNlJ1gKfBX5hylM6qZK8BThYVXuTbJv2fKbojVV1IMnPArcm\n+a/xnSfyOliJZ/oHgI1j2xuGsY6+neRMgOHfg1Oez7JLsppR8P++qj4zDLdbh6p6ErgdeD2wNsmh\nE7RT/fXwBuCtSb7J6NLu+cBf0msNqKoDw78HGf3w38qEXgcrMfp3AZuHd+vXAO8A9kx5TtOyB7hk\nuH0J8M9TnMuyG67d/i3wcFX9+diuFuuQZGY4wyfJTwO/zuh9jduB3xkOO2WfP0BVva+qNlTVJkav\n/S9W1TtptAZJfibJCw/dBt4MPMCEXgcr8o+zklzI6LreacDuqrpmylNadkluALYx+j/pfRu4Cvgc\ncBPwUkb/59G3V9XCN3tPGUneCPw7cD//fz33jxld1z/l1yHJWYzeoDuN0QnZTVV1dZJXMDrrfQlw\nN/B7VfWD6c305Bgu71xRVW/ptAbDc/3ssLkK+IequibJ6UzgdbAioy9JWh4r8fKOJGmZGH1JasTo\nS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpkf8DzhisI0l6ycsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgqgJeiZbbDL",
        "colab_type": "text"
      },
      "source": [
        "## Problem 4\n",
        "\n",
        "Repeat exercise (3) but use k-NN instead of parzen windows. Use different\n",
        "values of k and choose the one that results in the best error performance of the classifier.\n",
        "Make a plot to justify your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcOHyF-6NKMg",
        "colab_type": "code",
        "outputId": "5b1a0ebf-6371-41ba-9eee-e6943ce61b4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train) \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfKeYfqwmV9H",
        "colab_type": "code",
        "outputId": "19912f09-4c44-48eb-aeb6-c8cdc5efaa90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "A = []\n",
        "\n",
        "for i in range(2, 50):\n",
        "  \n",
        "  knn = KNeighborsClassifier(n_neighbors=i)\n",
        "  knn.fit(X_train, y_train) \n",
        "  \n",
        "  y_hat = knn.predict(X_test)\n",
        "  \n",
        "  #am I supposed to use this as a P for bayesian classfiication or just proceed\n",
        "  \n",
        "  error = calculate_error(y_hat, y_test)\n",
        "  \n",
        "  A.append(np.array([i, error]))\n",
        "  \n",
        "\n",
        "A = np.vstack(A)  \n",
        "lowest = np.where(A == np.amin(A[:,1]))\n",
        "\n",
        "\n",
        "plt.plot(A[:,0], A[:,1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jrPcDRE1TP4",
        "colab_type": "code",
        "outputId": "bf446936-9abd-44f3-9289-d936cada7494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "A"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.        ,  0.08308308],\n",
              "       [ 3.        ,  0.07607608],\n",
              "       [ 4.        ,  0.06706707],\n",
              "       [ 5.        ,  0.06906907],\n",
              "       [ 6.        ,  0.06706707],\n",
              "       [ 7.        ,  0.06906907],\n",
              "       [ 8.        ,  0.06706707],\n",
              "       [ 9.        ,  0.07007007],\n",
              "       [10.        ,  0.06706707],\n",
              "       [11.        ,  0.06706707],\n",
              "       [12.        ,  0.06206206],\n",
              "       [13.        ,  0.06306306],\n",
              "       [14.        ,  0.05905906],\n",
              "       [15.        ,  0.06206206],\n",
              "       [16.        ,  0.05905906],\n",
              "       [17.        ,  0.05905906],\n",
              "       [18.        ,  0.06006006],\n",
              "       [19.        ,  0.06006006],\n",
              "       [20.        ,  0.05705706],\n",
              "       [21.        ,  0.05705706],\n",
              "       [22.        ,  0.05805806],\n",
              "       [23.        ,  0.06006006],\n",
              "       [24.        ,  0.05805806],\n",
              "       [25.        ,  0.05905906],\n",
              "       [26.        ,  0.06006006],\n",
              "       [27.        ,  0.05905906],\n",
              "       [28.        ,  0.05805806],\n",
              "       [29.        ,  0.05705706],\n",
              "       [30.        ,  0.05705706],\n",
              "       [31.        ,  0.06006006],\n",
              "       [32.        ,  0.06106106],\n",
              "       [33.        ,  0.06006006],\n",
              "       [34.        ,  0.05905906],\n",
              "       [35.        ,  0.05805806],\n",
              "       [36.        ,  0.05705706],\n",
              "       [37.        ,  0.05705706],\n",
              "       [38.        ,  0.05805806],\n",
              "       [39.        ,  0.05805806],\n",
              "       [40.        ,  0.05805806],\n",
              "       [41.        ,  0.05605606],\n",
              "       [42.        ,  0.05705706],\n",
              "       [43.        ,  0.05705706],\n",
              "       [44.        ,  0.06006006],\n",
              "       [45.        ,  0.06006006],\n",
              "       [46.        ,  0.05905906],\n",
              "       [47.        ,  0.06006006],\n",
              "       [48.        ,  0.06206206],\n",
              "       [49.        ,  0.05805806]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNHXXYBU6gU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}